# 提案書：AI駆動型 次世代データ分析プラットフォーム構想（表現調整版）
## ～「ブラックボックス」からの脱却と、データの「由来（素性）」が確認できる信頼の基盤へ～

---

## 1. はじめに（Executive Summary）
貴社のデータ活用においては、インフラの老朽化に加え、長年の運用の積み重ねにより **処理の全体像が把握しづらくなっていること（ブラックボックス化）** や、それに伴う **データの信頼性の確認に時間がかかること** が、重要な課題になっていると理解しております。

本提案では、Vertica および既存の ETL（Extract, Transform, Load：データの抽出・変換・格納）処理（Python／独自ツール）のクラウド移行を契機に、近年発展の著しい **生成AI（文章や要約を生成できるAI）** と **データカタログ（データの定義や所在を整理する台帳）** を組み合わせます。  
これにより、処理内容の見える化（ホワイトボックス化）を進め、Tableauユーザーの皆様が **「このデータは何を意味し、どこから来て、いつ更新されたか」** を即座に確認できる、**「透明性が高く、運用負荷を抑えたデータ基盤」** の実現を目指します。

---

## 2. 現状の課題と仮説（As-Is Challenges）
ヒアリングおよび現状分析に基づき、貴社のデータ環境における主要な論点を以下のように整理しました。

| 領域 | 課題の要点 | 業務への影響（例） |
|---|---|---|
| ① データパイプライン（処理フロー） | **Pythonや独自ツールの処理が複雑化し、全体像が把握しづらい**<br>処理ロジックがドキュメントとして十分に整備されておらず、担当者以外が理解・改修しづらい状況。 | ・改修時の影響調査や引き継ぎに時間がかかる。<br>・障害発生時に、原因が「処理（コード）」か「入力データ」か切り分けるのに時間を要し、復旧が遅れやすい。 |
| ② データベース基盤（Vertica） | **データの意味（ビジネス上の定義）が整理されていない**<br>テーブル名・カラム名が技術的表記のままの箇所があり、利用者にとって「何のデータか」が分かりづらい。 | ・類似テーブルが複数存在し、どれを参照すべきか判断が難しい。<br>・分析着手までの「正しいデータ探し」に多くの時間が割かれる。 |
| ③ アプリケーション（Tableau） | **数値の根拠（算出方法・更新情報）を確認しづらい**<br>ダッシュボードの数値に疑問が生じた際、定義や更新日時、参照元を利用者が自己解決しにくい。 | ・「この数値の根拠は？」という問い合わせが増え、IT部門の対応負荷が高まりやすい。<br>・確認のために手作業（例：Excelでの再集計）が発生し、ダッシュボードが意思決定で使われにくくなる。 |

---

## 3. 目指す世界観と提供価値（Value Proposition）
本プロジェクトでは、AIエージェント（人の代わりに調査・要約・提案を支援するAI）を活用し、以下の3つの価値を提供します。

### 価値1：パイプラインの「見える化」と自動カタログ化
既存のPythonコードや独自設定ファイルをAIが解析し、**「日本語で読める説明（仕様メモ／カタログ）」** と **データリネージ（データの流れ・つながり）** を自動生成します。  
処理の変更に合わせてドキュメントも更新される仕組みを整え、属人化を抑えながら、常に最新の状態で全体像を把握できる環境を構築します。

### 価値2：運用負荷の低減を目指す「AI支援運用（AIOps：AIによる運用支援）」
単なる移行にとどまらず、運用の効率化を推進します。  
AIがログ（記録）を監視し、障害時に「基盤（インフラ）側の問題か、データ／処理側の問題か」を切り分ける支援を行います。さらに、データ量の増減に応じてリソース（計算資源）を適切に調整するなど、**人手を減らしながら安定稼働を支える仕組み** を検討します。

### 価値3：Tableau上で「データの由来（素性）」を確認できる仕組み
Tableauは継続利用したまま、Tableauの画面から **データ定義・参照元・更新日時** などを確認できる導線を用意します。  
API（Application Programming Interface：システム間連携のための窓口）を通じてカタログ情報を表示し、利用者が「この数値の定義は？」「いつ更新された？」をその場で確認できるようにすることで、意思決定に必要な信頼性を高めます。

---

## 4. 推奨プラットフォーム選定（Tool Selection）
貴社の要件（Python資産の活用、Tableau連携、カタログ重視）を踏まえ、主要3サービスを比較しました。

> ※各サービス名は製品名のため原則そのまま記載しています。用語は初出で補足を付けています。

| 評価項目 | Databricks（推奨） | Snowflake（次点） | Microsoft Fabric |
|---|---|---|---|
| ① Python解析とカタログ化 | ◎ **適合度が高い**（Unity Catalog：Databricksのメタデータ／権限管理機能）<br>データのつながり（リネージ）を自動的に整理しやすく、既存Python資産の活用にも相性が良い。 | ○ **対応可能**（Snowpark：Snowflake上でPython等を実行する仕組み）<br>Pythonは扱えるが、複雑な依存関係の見える化は外部ツール併用など工夫が必要な場合がある。 | △ **方針が異なる**<br>ローコード（画面操作中心）との親和性が高く、既存の複雑なPython処理の移行・解析は追加設計が必要になりやすい。 |
| ② Tableau連携と素性表示API | ◎ **APIが充実**<br>カタログ情報の取得手段が用意されており、Tableau拡張機能等との連携設計がしやすい。 | ◎ **連携しやすい**<br>SQL（データ操作言語）中心でメタデータを取得しやすく、構成は組みやすい。 | △ **Power BI中心の設計になりやすい**<br>Tableauから同等のメタデータ連携を行う場合、構成が複雑になりやすい。 |
| ③ Verticaからの移行性 | ○ **移行可能**<br>SQLウェアハウス（SQLで分析できる基盤）として性能を出しやすいが、物理設計の考え方は一部見直しが必要。 | ◎ **移行しやすい**<br>アーキテクチャ（構造）が近く、SQL移行リスクは比較的低い。 | ○ **移行可能**<br>要件により性能面のチューニング（調整）が必要になる可能性がある。 |

### ★ 最終推奨：Databricks（Data Intelligence Platform）
**理由：** 貴社の最重要課題である **「Python／独自ツールの処理が把握しづらい」** 状態を改善するには、リネージやカタログ整備を一体で進められる機能群が有効です。Databricks（特にUnity Catalog）を中心に据えることで、可視化・標準化・運用効率化（AIOps支援）を同時に進めやすくなります。

> ※SQL中心の保守体制を強く重視される場合は、Snowflakeも有力な選択肢です。

---

## 5. プロジェクト計画とWBS（Implementation Plan）
**3ヶ月間** の構想策定および PoC（Proof of Concept：概念実証）フェーズの計画です。  
WBS（Work Breakdown Structure：作業分解構成）として、段階ごとの目的と作業項目を整理します。

### Phase 1：パイプライン層（処理のカタログ化とAI活用の検証）
**目的：** 既存資産の棚卸しと、AIによる可視化・最適化が有効かを確認します。

- **1.1 現行コードのAI解析・可視化**  
  Python／独自ツール設定をAIに読み込ませ、処理の概要説明とデータの流れ（リネージ）を整理します。
- **1.2 パイプラインカタログ設計**  
  処理単位（ジョブ）の標準化と、メタデータ（責任者、更新頻度、SLA（Service Level Agreement：サービス品質の合意水準）等）の定義方針を策定します。
- **1.3 AIによるリファクタリング（改修）・自動化検証（PoC）**  
  既存コードを標準的な実行基盤（例：Databricks Workflows：ジョブ実行の仕組み）へ整理・移植できるかを検証し、カタログと連動させる方法を確認します。
- **1.4 運用プロセス設計（AIOps支援）**  
  ログ監視、アラート（通知）設計、一次切り分け支援のルールを整理します。

### Phase 2：データベース基盤層（データ資産のカタログ整理）
**目的：** Vertica移行設計と、利用者が理解しやすいデータ定義（意味付け）を整備します。

- **2.1 カタログメタデータ整備構想**  
  ビジネス用語集（Glossary：業務用語の辞書）を整備し、物理カラム（実際の列）との対応付け方針を策定します。
- **2.2 Vertica移行とカタログ連携**  
  移行対象データの選定と、Unity Catalog等へのメタデータ同期（反映）設計を行います。
- **2.3 データ品質（Quality：品質）定義**  
  「信頼できるデータ」の認定基準を策定し、品質監視（例：Lakehouse Monitoring：データ品質や更新状況の監視）設定の方針を定めます。

### Phase 3：アプリケーション層（BI＋データ由来確認の窓口）
**目的：** Tableauと連携し、利用者が数値の根拠を自分で確認できる仕組みを作ります。

- **3.1 「データ由来（素性）」UX設計**  
  Tableauダッシュボード上で、定義・更新日時・参照元などを表示する画面導線（UI/UX：利用者体験）を設計します。
- **3.2 RAG基盤設計**  
  RAG（Retrieval-Augmented Generation：検索結果を参照して回答を生成する仕組み）により、利用者の質問（例：「この数字の定義は？」）に対し、カタログを参照して回答するAIエージェントを設計します。
- **3.3 プロトタイプ検証**  
  Tableau画面とバックエンド（裏側）のメタデータ連携を行い、実機で動作を確認します。

---

## 6. 成果物一覧（Deliverables）
本フェーズ完了時に、以下の成果物を納品いたします。これにより、次フェーズ（構築・移行）へスムーズに移行可能です。

- AI解析済み パイプラインカタログ＆リネージ図（現状の可視化レポート）
- 次期データ分析基盤アーキテクチャ設計書（Databricks／Snowflake構成図）
- データカタログ・ビジネス用語集（ドラフト版）
- AIOps（AIによる運用支援）要件定義書
- Tableau連携・データ由来（素性）表示API仕様書
- 移行ロードマップおよび費用対効果（ROI：投資対効果）試算書

---

## 7. 結び（Conclusion）
本プロジェクトは、単にシステムをクラウドへ移行するだけでなく、AIの力を活用して、長年の運用で複雑化した処理や定義を **「整理された説明可能な資産（カタログ）」** として整える取り組みです。

利用者の皆様がデータの由来（素性）を理解し、安心してデータを使える環境を整えることで、貴社のデータに基づく意思決定を、より確かなものへと高めていくことを目指します。ぜひ、この変革をご一緒させてください。

---

## 参考：本書で用いた用語（抜粋）
- **ETL**：Extract, Transform, Load（データの抽出・変換・格納）
- **PoC**：Proof of Concept（概念実証）
- **WBS**：Work Breakdown Structure（作業分解構成）
- **データカタログ**：データの定義・所在・責任者などを整理する台帳
- **データリネージ**：データがどこから来て、どの処理を経て、どこへ行くかの流れ
- **API**：Application Programming Interface（システム間連携のための窓口）
- **AIOps**：AIによる運用支援（ログ監視、原因切り分け支援など）
- **RAG**：検索結果を参照して回答を生成する仕組み
- **SLA**：Service Level Agreement（サービス品質の合意水準）
- **BI**：Business Intelligence（業務データを可視化・分析して意思決定を支援する仕組み）
- **ROI**：Return on Investment（投資対効果）

---

> **注記**  
> 生成AIの出力は誤りを含む可能性があるため、重要な判断に用いる前に内容の確認をお願いします。
