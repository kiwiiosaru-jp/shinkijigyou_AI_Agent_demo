# 提案書：AI駆動型 次世代データ分析プラットフォーム構想
## ～「ブラックボックス」からの脱却と、データの「素性」が見える信頼の基盤へ～

---

## 1. はじめに（Executive Summary）
貴社のデータ活用における課題は、インフラの老朽化以上に、長年の運用で複雑化した **「処理のブラックボックス化」と、それに起因する「データの信頼性欠如」** にあります。

本提案では、Vertica および既存ETL（Python/独自ツール）のクラウド移行を契機として、最新の **「生成AI」技術** と **「データカタログ」技術** を融合させます。これにより、システムの中身を可視化（ホワイトボックス化）し、Tableauユーザーが **「データの正しさ（素性）」** を即座に確認できる、**「透明で、自律したデータ基盤」** を実現します。

---

## 2. 現状の課題と仮説（As-Is Challenges）
ヒアリングおよび現状分析に基づき、貴社のデータ環境における「真の課題」を以下のように特定しました。

| 領域 | 課題の核心（Pain Points） | 業務への影響 |
|---|---|---|
| ① データパイプライン（処理フロー） | **「属人化したPythonの迷宮」**<br>長年積み上げられたPythonスクリプトや独自ツールの処理ロジックが、作成者以外には解読不能な「秘伝のタレ」となっている。 | ・改修時の影響調査に膨大な工数がかかる。<br>・障害発生時、「コードバグ」か「データ不良」かの切り分けができず、復旧が遅れる。 |
| ② データベース基盤（Vertica） | **「高速なデータの墓場」**<br>処理速度は速いが、テーブル名やカラム名が技術用語（col_001等）のままで、ビジネス的な意味が定義されていない。 | ・似たようなテーブルが乱立し、どれが正（Official）か不明。<br>・分析者が正しいデータを探すのに時間の8割を費やしている。 |
| ③ アプリケーション（Tableau） | **「信頼なき可視化」**<br>ダッシュボード等の数値に違和感があっても、その算出ロジックや更新日時（素性）を確認する術がない。 | ・「この数字合ってる？」という問い合わせがIT部門に殺到。<br>・結局、各自がExcelで再集計し、ダッシュボードが意思決定に使われない。 |

---

## 3. 目指す世界観と提供価値（Value Proposition）
本プロジェクトでは、AIエージェントを活用し、以下の3つの価値を提供します。

### 価値1：パイプラインの「ホワイトボックス化」と自動カタログ化
既存のPythonコードや独自設定ファイルをAIが解析し、**「日本語の仕様書（カタログ）」と「データリネージ（つながり）」** を自動生成します。  
「誰も読めなかった処理」を、AIが常に最新の状態でドキュメント化し続ける環境を構築します。

### 価値2："NoOps" を目指す「AI自律運用（AIOps）」
単なる移行ではなく、運用の自動化を目指します。AIがエラーログを監視し、「インフラ起因かデータ起因か」を即座に診断。さらに、データの増減に合わせてリソースを自動調整する **「自律型パイプライン」** を実現します。

### 価値3：Tableauへの「データ素性（Traceability）」注入
Tableauを捨てる必要はありません。Tableauの画面上に、API経由でデータカタログ情報を表示させます。ユーザーはダッシュボードを見ながら、  
「このデータの定義は？」「最終更新はいつ？」といったデータの素性をリアルタイムに確認でき、数字への絶対的な信頼を担保します。

---

## 4. 推奨プラットフォーム選定（Tool Selection）
貴社の要件（Python資産の活用、Tableau連携、カタログ重視）に基づき、主要3サービスを比較評価しました。

| 評価項目 | Databricks（推奨） | Snowflake（次点） | Microsoft Fabric |
|---|---|---|---|
| ① Python解析とカタログ化 | ◎ 最適（Unity Catalog）<br>実行時のコンテキスト（変数の動き）を含めてリネージを自動生成可能。AIによるリファクタリング精度が最も高い。 | ○ 可能（Snowpark）<br>Pythonは動作するが、複雑な処理の依存関係可視化には外部ツール等の工夫が必要。 | △ 不向き<br>ローコード志向が強く、既存の複雑なPythonコードの解析・移行には不向き。 |
| ② Tableau連携と素性表示API | ◎ APIファースト<br>カタログ情報取得APIが充実しており、Tableau拡張機能との親和性が高い。 | ◎ 強力<br>SQLベースでメタデータを取得しやすく、連携は容易。 | △ Power BI前提<br>Tableauからメタデータを取得する構成が複雑になりがち。 |
| ③ Verticaからの移行性 | ○ 良好<br>SQLウェアハウスにより高い性能を発揮。物理設計の思想変更は多少必要。 | ◎ 最適<br>アーキテクチャが似ており、SQL移行リスクは最も低い。 | ○ 普通<br>性能面でのチューニングが必要になる可能性がある。 |

### ★ 最終推奨：Databricks（Data Intelligence Platform）
**理由：** 貴社の最大の課題である **「Python/独自ツールのブラックボックス化」** を解消するには、Unity Catalog の強力なリネージ自動生成機能と、PythonネイティブなAI解析能力が不可欠です。これにより、運用工数の劇的な削減（AIOps）が見込めます。

> ※SQL中心の保守体制を重視される場合は、Snowflakeも有力な選択肢となります。

---

## 5. プロジェクト計画とWBS（Implementation Plan）
**3ヶ月間** の構想策定およびPoC（概念実証）フェーズの計画です。

### Phase 1：パイプライン層（処理のカタログ化とAI自動化）
**目的：** 既存資産の棚卸しと、AIによる可視化・最適化の実証。

- **1.1 現行コードのAI解析・可視化**  
  Python/独自ツール設定値をLLMに読み込ませ、処理ロジック要約とリネージ図を作成。
- **1.2 パイプラインカタログ設計**  
  処理単位（ジョブ）の標準化と、メタデータ定義（責任者、SLA等）。
- **1.3 AIリファクタリング・自動化検証（PoC）**  
  スパゲッティコードをAIで標準FW（Databricks Workflows等）へ変換し、カタログと自動連動させる検証。
- **1.4 自動化運用プロセス設計（AIOps）**  
  AIによるエラー診断・自動通知ルールの策定。

### Phase 2：データベース基盤層（データ資産のカタログ整理）
**目的：** Vertica移行設計と、意味のあるデータカタログの構築。

- **2.1 カタログメタデータ整備構想**  
  ビジネス用語集（Glossary）の策定と、物理カラムとの紐付け。
- **2.2 Vertica移行とカタログ連携**  
  移行対象データの選定と、Unity Catalog等へのメタデータ同期設計。
- **2.3 データ品質（Quality）定義**  
  「信頼できるデータ」の認定基準策定と、AIによる品質監視（Lakehouse Monitoring）設定。

### Phase 3：アプリケーション層（BI ＋ データ素性提供窓口）
**目的：** Tableauと連携した「信頼性確認」の仕組み作り。

- **3.1 「データ素性」UX設計**  
  Tableauダッシュボード上でのカタログ情報表示（API連携）の画面設計。
- **3.2 RAG（検索拡張生成）基盤設計**  
  ユーザーの質問（「この数字の定義は？」）に対し、カタログを参照して回答するAIエージェントの設計。
- **3.3 プロトタイプ検証**  
  実際のTableau画面とバックエンドのメタデータを連携させた実機検証。

---

## 6. 成果物一覧（Deliverables）
本フェーズ完了時に、以下の成果物を納品いたします。これにより、次フェーズ（構築・移行）へスムーズに移行可能です。

- AI解析済み パイプラインカタログ＆リネージ図（現状の可視化レポート）
- 次期データ分析基盤アーキテクチャ設計書（Databricks/Snowflake構成図）
- データカタログ・ビジネス用語集（ドラフト版）
- AIOps（自動運用）要件定義書
- Tableau連携・データ素性表示API仕様書
- 移行ロードマップおよび費用対効果（ROI）試算書

---

## 7. 結び（Conclusion）
本プロジェクトは、単にシステムをクラウドへ移すだけではありません。AIの力を借りて、長年積み重なった **「技術的負債（ブラックボックス）」** を **「価値ある資産（カタログ）」** へと昇華させる取り組みです。

ユーザーがデータの「素性」を理解し、安心してデータを使える環境を作ることで、貴社のデータドリブン経営を次のステージへ引き上げます。ぜひ、この変革をご一緒させてください。

---

> **注記**  
> Google AI models may make mistakes, so double-check outputs.
